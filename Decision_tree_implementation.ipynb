{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## üåü Smart Modality Selection in Camel Dataset üê´\n",
        "\n",
        "This code implements a decision tree algorithm in the Camel dataset using two modalities. The algorithm dynamically selects the modality deemed better for object tracking initiation until the end of tracking.\n",
        "\n",
        "If the tracker loses the object for more than 3 consecutive frames, it intelligently switches to utilizing frames from the alternative modality.\n",
        "\n",
        "Let's optimize your object tracking with smart modality selection! üöÄ\n"
      ],
      "metadata": {
        "id": "amySGOP0PKaT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smJi8FEQNkbp"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "# Suppress UserWarning related to feature names\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"X does not have valid feature names*\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, recall_score, roc_auc_score, f1_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.neural_network import MLPClassifier"
      ],
      "metadata": {
        "id": "3WguP4JnNoeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù Setting Annotation Parameters üìÅ\n",
        "\n",
        "Set the variable `name` to the desired name of the result annotation file, and `path` to where the Camel dataset is located:\n",
        "\n",
        "```python\n",
        "name = 'TrackerCSRT_random_forest_dynamic'  # üéØ Desired name for the result annotation file\n",
        "path = '/content/drive/MyDrive/Camel'  # üê™ Path to the Camel dataset\n"
      ],
      "metadata": {
        "id": "8NeGCGwqQ3qt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name = 'TrackerCSRT_random_forest_dynamic'\n",
        "path = '/content/drive/MyDrive/Camel'"
      ],
      "metadata": {
        "id": "SV6MWK6jNqPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üõ†Ô∏è Loading Pre-Trained Model Components!\n",
        "\n",
        "Unleash the might of your pre-trained scaler, imputer, and model! üí•\n"
      ],
      "metadata": {
        "id": "aAcJ8p8MRWKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the scaler and imputer from files\n",
        "scaler = joblib.load(f'{path}/scaler.pkl')\n",
        "imputer = joblib.load(f'{path}/imputer.pkl')\n",
        "model = joblib.load(f'{path}/model.pkl')"
      ],
      "metadata": {
        "id": "5MUUU-b1Nr0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_bbox_data_from_dataframe(dataframe):\n",
        "    bbox_data = {}\n",
        "    latest_frames = {}\n",
        "    for index, row in dataframe.iterrows():\n",
        "        frame = int(row['frame_id'])\n",
        "        track_id = int(row['track_id'])\n",
        "        class_id = int(row['class_id'])\n",
        "        bbox = [int(row['absolute_gt_x']), int(row['absolute_gt_y']), int(row['absolute_gt_width']), int(row['absolute_gt_height'])]\n",
        "        bbox_data.setdefault(frame, []).append((track_id, bbox))\n",
        "        latest_frames[track_id] = max(frame, latest_frames.get(track_id, 0))  # Update the latest frame for the track_id\n",
        "    return bbox_data, latest_frames\n",
        "\n",
        "def generate_absolute_truth(vis_file,ir_file):\n",
        "    # Read the tab-delimited text file into a pandas DataFrame\n",
        "    df1 = pd.read_csv(vis_file, delimiter='\\t', header=None, names=['frame_id','track_id', 'class', 'x', 'y', 'width', 'height'], dtype={'frame_id': int, 'track_id': int,'class': int, 'x': float, 'y': float, 'width': float, 'height': float}, on_bad_lines='skip')\n",
        "    df2 =  pd.read_csv(ir_file, delimiter='\\t', header=None, names=['frame_id', 'track_id','class', 'x', 'y', 'width', 'height'], dtype={'frame_id': int, 'track_id': int,'class': int, 'x': float, 'y': float, 'width': float, 'height': float}, on_bad_lines='skip')\n",
        "    # Perform a full outer join on the two DataFrames\n",
        "    merged_df = pd.merge(df1, df2, on=['frame_id', 'track_id'], how='outer', suffixes=('_vis', '_ir'))\n",
        "\n",
        "    # Determine the bigger bounding box for each row and preserve only x, y, width, height\n",
        "    merged_df['absolute_gt_x'] = merged_df.apply(lambda row: min(row['x_vis'], row['x_ir']) if not pd.isna(row['x_vis']) and not pd.isna(row['x_ir']) else row['x_vis'] if not pd.isna(row['x_vis']) else row['x_ir'], axis=1)\n",
        "    merged_df['absolute_gt_y'] = merged_df.apply(lambda row: min(row['y_vis'], row['y_ir']) if not pd.isna(row['y_vis']) and not pd.isna(row['y_ir']) else row['y_vis'] if not pd.isna(row['y_vis']) else row['y_ir'], axis=1)\n",
        "    merged_df['absolute_gt_width'] = merged_df.apply(lambda row: max(row['x_vis'] + row['width_vis'], row['x_ir'] + row['width_ir']) - min(row['x_vis'], row['x_ir']) if not pd.isna(row['x_vis']) and not pd.isna(row['x_ir']) else row['width_vis'] if not pd.isna(row['width_vis']) else row['width_ir'], axis=1)\n",
        "    merged_df['absolute_gt_height'] = merged_df.apply(lambda row: max(row['y_vis'] + row['height_vis'], row['y_ir'] + row['height_ir']) - min(row['y_vis'], row['y_ir']) if not pd.isna(row['y_vis']) and not pd.isna(row['y_ir']) else row['height_vis'] if not pd.isna(row['height_vis']) else row['height_ir'], axis=1)\n",
        "\n",
        "    merged_df['class_id'] = -1\n",
        "\n",
        "    # Drop all other columns except frame_id, track_id, and absolute_gt\n",
        "    merged_df = merged_df[['frame_id', 'track_id','class_id', 'absolute_gt_x', 'absolute_gt_y', 'absolute_gt_width', 'absolute_gt_height']]\n",
        "\n",
        "    return merged_df\n",
        "\n",
        "def adjust_bounding_box(frame, roi):\n",
        "    height, width, _ = frame.shape\n",
        "    height -= 1\n",
        "    width -= 1\n",
        "    x, y, w, h = roi\n",
        "\n",
        "    # Adjust x coordinate if it's less than 0\n",
        "    if x < 0:\n",
        "        w += x  # Decrease width\n",
        "        x = 0   # Set x to 0\n",
        "\n",
        "    # Adjust width if it exceeds the frame width\n",
        "    if x + w > width:\n",
        "        w = width - x\n",
        "\n",
        "    # Adjust y coordinate if it's less than 0\n",
        "    if y < 0:\n",
        "        h += y  # Decrease height\n",
        "        y = 0   # Set y to 0\n",
        "\n",
        "    # Adjust height if it exceeds the frame height\n",
        "    if y + h > height:\n",
        "        h = height - y\n",
        "\n",
        "    # Ensure width and height are at least 5\n",
        "    w = max(5, w)\n",
        "    h = max(5, h)\n",
        "\n",
        "    # Adjust width if it exceeds the frame width\n",
        "    if x + w > width:\n",
        "        x = width - w\n",
        "\n",
        "    # Adjust height if it exceeds the frame height\n",
        "    if y + h > height:\n",
        "        y = height - h\n",
        "\n",
        "\n",
        "    return (x, y, w, h)"
      ],
      "metadata": {
        "id": "J_oiDGCLOTBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_sample(features, scaler, imputer, model):\n",
        "    # Apply imputation\n",
        "    features_imputed = imputer.transform([features])\n",
        "\n",
        "    # Apply scaling\n",
        "    features_scaled = scaler.transform(features_imputed)\n",
        "\n",
        "    # Use the model to make predictions\n",
        "    prediction = model.predict(features_scaled)\n",
        "\n",
        "    return prediction"
      ],
      "metadata": {
        "id": "OJ-usHcvNzF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_image_features(frame, frame2, x, y, width, height):\n",
        "    # frame is visible\n",
        "    # frame2 is IR\n",
        "    frame_copy = frame.copy()\n",
        "    frame2_copy = frame2.copy()\n",
        "    roi = frame_copy[y:y+height, x:x+width]\n",
        "    roi2 = frame2_copy[y:y+height, x:x+width]\n",
        "\n",
        "    frame_gray = cv2.cvtColor(frame_copy, cv2.COLOR_BGR2GRAY)\n",
        "    frame2_gray = cv2.cvtColor(frame2_copy, cv2.COLOR_BGR2GRAY)\n",
        "    roi_gray = frame_gray[y:y+height, x:x+width]\n",
        "    roi2_gray = frame2_gray[y:y+height, x:x+width]\n",
        "\n",
        "    rois = [(frame_gray, frame_copy),\n",
        "            (frame2_gray, frame2_copy),\n",
        "            (roi_gray, roi),\n",
        "            (roi2_gray, roi2)]\n",
        "\n",
        "    features = []\n",
        "\n",
        "    for x in rois:\n",
        "        gray_roi = x[0]\n",
        "        roi = x[1]\n",
        "\n",
        "        # Convert the cropped region to BGR\n",
        "        b, g, r = cv2.split(roi)\n",
        "\n",
        "        mean_intensity_b = np.mean(b)\n",
        "        mean_intensity_g = np.mean(g)\n",
        "        mean_intensity_r = np.mean(r)\n",
        "\n",
        "        # Compute median intensity directly from pixel values\n",
        "        median_intensity_b = np.median(b)\n",
        "        median_intensity_g = np.median(g)\n",
        "        median_intensity_r = np.median(r)\n",
        "\n",
        "        # Compute mode intensity directly from pixel values\n",
        "        mode_intensity_b = np.argmax(np.bincount(b.flatten()))\n",
        "        mode_intensity_g = np.argmax(np.bincount(g.flatten()))\n",
        "        mode_intensity_r = np.argmax(np.bincount(r.flatten()))\n",
        "\n",
        "        # Compute standard deviation directly from pixel values\n",
        "        std_deviation_b = np.std(b)\n",
        "        std_deviation_g = np.std(g)\n",
        "        std_deviation_r = np.std(r)\n",
        "\n",
        "        # Compute intensity statistics\n",
        "        mean_intensity = np.mean(gray_roi)\n",
        "        variance_intensity = np.var(gray_roi)\n",
        "        skewness_intensity = np.mean((gray_roi - mean_intensity) ** 3) / np.power(variance_intensity, 1.5)\n",
        "        kurtosis_intensity = np.mean((gray_roi - mean_intensity) ** 4) / np.power(variance_intensity, 2) - 3\n",
        "\n",
        "        area = width*height\n",
        "\n",
        "        # Include all variables in the features list\n",
        "        features += [mean_intensity_b, mean_intensity_g, mean_intensity_r,\n",
        "                median_intensity_b, median_intensity_g, median_intensity_r,\n",
        "                mode_intensity_b, mode_intensity_g, mode_intensity_r,\n",
        "                std_deviation_b, std_deviation_g, std_deviation_r,\n",
        "                mean_intensity, variance_intensity, skewness_intensity, kurtosis_intensity, area]\n",
        "\n",
        "    # Return all features\n",
        "    return features"
      ],
      "metadata": {
        "id": "nFlCtVs-N1x9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def annotate(vid_in_vis, vid_in_ir, annotation_in_vis, annotation_in_ir, annotation_file_out, annotation_file_out2):\n",
        "    trackers = {}\n",
        "    # Process frames and initialize trackers\n",
        "    cap = cv2.VideoCapture(vid_in_vis)\n",
        "    cap2 = cv2.VideoCapture(vid_in_ir)\n",
        "\n",
        "    annotation_df = generate_absolute_truth(annotation_in_vis, annotation_in_ir)\n",
        "    bbox_data, latest_frames = read_bbox_data_from_dataframe(annotation_df)\n",
        "\n",
        "    with open(annotation_file_out, 'w') as file, open(annotation_file_out2, 'w') as file2:\n",
        "        while cap.isOpened() and cap2.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            ret2, frame2 = cap2.read()\n",
        "            if not ret or not ret2:\n",
        "                break\n",
        "\n",
        "            frame_number = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
        "\n",
        "            # Initialize trackers at the earliest frame where track_id appears\n",
        "            if frame_number in bbox_data:\n",
        "                for track_id, bbox in bbox_data[frame_number]:\n",
        "                    if track_id not in trackers:\n",
        "                        tracker3 = cv2.TrackerCSRT_create()\n",
        "                        image_shape = frame.shape\n",
        "                        bbox = adjust_bounding_box(frame2, bbox)\n",
        "                        if bbox[2] <= 0 or bbox[3] <= 0:\n",
        "                            continue\n",
        "                        x, y, w, h = bbox\n",
        "                        features = extract_image_features(frame, frame2, x, y, w, h)\n",
        "                        ir_is_better = eval_sample(features, scaler, imputer, model)\n",
        "                        if ir_is_better:\n",
        "                            tracker3.init(frame2, tuple(bbox))\n",
        "                        else:\n",
        "                            tracker3.init(frame, tuple(bbox))\n",
        "\n",
        "                        trackers[track_id] = {'tracker_dual': tracker3, 'last_bbox': bbox, 'lost_count': 0, 'ir_better': ir_is_better}\n",
        "\n",
        "            # Update trackers and draw bounding boxes\n",
        "            for track_id, data in trackers.items():\n",
        "\n",
        "                # Here I tried to determine better modality for each frame -> this only confused the tracker resulting in worse performace\n",
        "\n",
        "                #if 0 not in data['last_bbox'][2:] and not any(num < 0 for num in data['last_bbox'][:2]):\n",
        "                #    x, y, w, h = data['last_bbox']\n",
        "                #    features = extract_image_features(frame, frame2, x, y, w, h)\n",
        "                #    data['ir_better'] = eval_sample(features, scaler, imputer, model)\n",
        "\n",
        "                if data['ir_better']:\n",
        "                    frame3 = frame2\n",
        "                    frame4 = frame\n",
        "                else:\n",
        "                    frame3 =  frame\n",
        "                    frame4 = frame2\n",
        "\n",
        "                success3, bbox3 = data['tracker_dual'].update(frame3)\n",
        "\n",
        "                if not success3:\n",
        "                    data['lost_count'] += 1\n",
        "                    if data['lost_count'] > 3:\n",
        "\n",
        "                        if 0 not in data['last_bbox'][2:] and not any(num < 0 for num in data['last_bbox'][:2]):\n",
        "                            x, y, w, h = data['last_bbox']\n",
        "                            features = extract_image_features(frame, frame2, x, y, w, h)\n",
        "                            data['ir_better'] = eval_sample(features, scaler, imputer, model)\n",
        "                            if data['ir_better']:\n",
        "                                frame4 = frame\n",
        "                            else:\n",
        "                                frame4 = frame2\n",
        "\n",
        "                        success, bbox3 = data['tracker_dual'].update(frame4)\n",
        "                else:\n",
        "                    data['lost_count'] = 0\n",
        "\n",
        "                if success3:\n",
        "                    data['last_bbox'] = bbox3\n",
        "                else:\n",
        "                    bbox3 = data['last_bbox']\n",
        "\n",
        "\n",
        "\n",
        "                x1, y1, w, h = bbox3\n",
        "                file.write(f\"{frame_number} {track_id} {-1} {x1} {y1} {w} {h}\\n\")\n",
        "                file2.write(f\"{frame_number} {track_id} {-1} {x1} {y1} {w} {h}\\n\")\n",
        "\n",
        "            for track_id, value in latest_frames.items():\n",
        "                if value == frame_number: # or track_id in to_remove\n",
        "                    # Delete the tracker associated with the current track ID\n",
        "                    del trackers[track_id]\n",
        "\n",
        "            # Press 'q' to quit\n",
        "            if cv2.waitKey(25) & 0xFF == ord('q'):\n",
        "                break\n",
        "\n",
        "    # Release resources\n",
        "    cap.release()\n",
        "    cap2.release()\n",
        "    cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "Iouk2p7xN2SE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üö´ Skipped Files Due to Missing Videos/Annotations\n",
        "\n",
        "Unfortunately, during processing, the following files had to be skipped due to missing videos or annotations:\n",
        "\n",
        "- File 12\n",
        "- File 14\n",
        "- File 16\n",
        "- File 22\n",
        "- File 24\n",
        "\n",
        "These files are excluded from the analysis as their corresponding videos or annotations are not available. üòû\n"
      ],
      "metadata": {
        "id": "mfqgzUDDRwqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tqdm(range(1, 31)):\n",
        "    if i in [22,24,16,14,12]:\n",
        "        continue\n",
        "    annotate(f'{path}/seq-{i}/Visual-seq{i}.mp4',\n",
        "             f'{path}/seq-{i}/IR-seq{i}.mp4',\n",
        "             f'{path}/seq-{i}/Seq{i}-Vis.txt',\n",
        "             f'{path}/seq-{i}/Seq{i}-IR.txt',\n",
        "             f'{path}/seq-{i}/{name}-Out_Seq{i}-Vis.txt',\n",
        "             f'{path}/seq-{i}/{name}-Out_Seq{i}-IR.txt')"
      ],
      "metadata": {
        "id": "tu2438R6O_3W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}